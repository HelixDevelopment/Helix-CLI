# Helix CLI Configuration

# Global settings
global:
  theme: "default"
  auto_confirm: false
  max_workers: 4
  log_level: "info"
  data_dir: "~/.config/Helix"
  cache_dir: "~/.cache/Helix"

# LLM Provider configurations
llm:
  # Llama.cpp configuration
  llamacpp:
    enabled: true
    model_path: "~/models/llama-2-7b-chat.gguf"
    context_size: 4096
    gpu_enabled: true
    gpu_layers: 35
    server_host: "localhost"
    server_port: 8080
    server_timeout: 30

  # Ollama configuration
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    default_model: "llama2"
    timeout: 30
    keep_alive: 300
    stream_enabled: true

  # OpenAI configuration
  openai:
    enabled: false
    api_key: ""
    base_url: "https://api.openai.com/v1"
    default_model: "gpt-4"
    timeout: 30

# User interface settings
ui:
  terminal_width: 80
  show_ascii_art: true
  color_scheme: "default"
  syntax_highlighting: true

# Security settings
security:
  enable_encryption: true
  require_authentication: false
  session_timeout: 3600

# Database settings
database:
  host: "localhost"
  port: 5432
  user: "helix"
  password: ""
  dbname: "helix"
  ssl_mode: "disable"